# 第04章 决策树
## 4.1、试证明对于不含冲突数据（即特征向量完全相同但标记不同）的训练集，必存在与训练集一致（即训练误差为0）的决策树。
    答：因为决策树是通过属性来划分，相同属性的样本最终肯定会进入相同的叶结点，一个叶结点只有一个分类。如果样本属性相同而分类不同，则会出现训练误差，决策树与训练集不一致；反之，决策树只会在当前样本集合是同一类或者所有属性相同时才会停止划分，最终得到训练误差为0的决策树。

## 4.7、图4.2是一个递归算法，若面临巨量数据，则决策树的层数会很深，使用递归方法易导致“栈”溢出，试使用“队列”数据结构，以参数MaxDepth控制树的最大深度，写出与图4.2等价、但不使用递归的决策树生成算法。 
    答：直接用递归会导致大量的临时变量被保存，当层数过深时会导致“栈”溢出。
    用队列对决策树采用广度优先来生成，用MaxDepth来控制树的最大层数。队列中每个元素代表着决策树的每个结点，它必要的属性有：样本集合D、属性集合A，当前层数指示depth，孩子结点指针集合children。队列一开始里面只有一个元素，就是最初包含所有样本的根结点。然后当队列不为空的时候开始循环，每次取出一个结点，判断是否需要划分，如果不要，就是一个叶结点；如果需要划分，那么找出最优划分属性，然后划分成n个子集，依次送入队列。继续循环，直到队列为空。是否需要划分有4个依据：当前所有样本属于一类、达到了MaxDepth的深度、当前属性集为空、当前所有样本在当前属性集所有属性上取值相同。这样就完成了限制深度的广度优先决策树的构建。算法如下所示：

    输入：	训练集D = {(x1, y1), (x2, y2), ..., (xm, ym)}

    属性集A = {a1, a2, ..., ad}
    
    最大深度MaxDepth

生成根结点root[D, A, 1, {}]；
生成队列queue（保存结点信息）；
将root放入队列queue；

    while queue ≠ Φ
     队列queue出队，令p = 出队结点；
     if p->D中样本全属于同一类别C then
          p标记为C类叶结点，continue；
     end if
     if p->depth = MaxDepth OR p->A = Φ OR p->D中样本在p->A上取值相同 then
          p标记为p->D中样本数最多的类的叶结点，continue；
     end if
     从p->A中选择最优划分属性；
     for 的每一个值 do
          令表示p->D中在上取值为的样本子集；
          生成子结点child[, p->A\{}, p->depth+1, {}]；
          p->children += child；
          if = Φ then
               child标记为p->D中样本数最多的类的叶结点；
          else
               将child放入队列queue；
          end if
     end for
    end while
    输出：	根结点root（即以root为根结点的一棵决策树）
