# 第10章降维与度量学习

## 10.2、令**err**、**err**∗分别表示最近邻分类器与贝叶斯最优分类器的期望错误率，试证明：

​							$err^*\leq err\leq err*(2-\frac{\vert y \vert}{\vert y\vert-1}\times err^*)$



答:由书上P226可知，$err=1-\sum\limits_{c\in y}P^2(c|x)$，$err^*=1-\max\limits_{c \in y}P(c|x)$。设$c^*=\arg\max\limits_{c \in y}P(c|x)$，则$err^*=1-P(c^*|x)$

（1）根据期望错误率的定义，$\sum P^2(c|x)$表示期望正确率，结果肯定不大于$\max\limits_{c \in y}P(c|x)$，并且由于$P(c^*|x)=\max\limits_{c \in y}P(c^*|x)$，可得$\sum P^2(c|x)\leq \max\limits_{c \in y}P(c|x)= P(c^*|x)$，所以$err^*\leq err$。

（2）当$P_{c \in y-c^*}(c|x)=\frac{err^*}{\vert y\vert -1}$（即x属于$c^*$之外的其他类的概率均等）时，$\sum_{c\in y-c^*}P^2(c|x)$可以取到最小值（即![img](file:////tmp/wps-zhan/ksohtml/wpsmd9HjF.png)），因此可得
$$
\begin{equation}
\begin{aligned}
err&=1-\sum P^2(c|x)\newline
&=1-(P^2(c^*|x)+\sum_{c \in y-c^*}P^2(c|x))\newline
&=(1+P(c^*|x))(1-P(c^*|x))-\sum_{c\in y-c^*}P^2(c|x)\newline
&=(2-err^*)err^*-\sum_{c\in y-c^*}P^2(c|x)\newline
&\leq(2-err^*)err^*-\frac{(err^*)^2}{\vert y\vert -1}\newline
&=err^*(2-\frac{\vert y\vert}{\vert y\vert -1}\times err^*)
\end{aligned}
\end{equation}
$$


。

 

 

## 10.3、在对高维数据降维前应先进行“中心化”，常见的是将协方差矩阵$XX^T$转换为$XHH^TX^T$，其中$H=I-1/mll^T$，试析其效果。

答：假设X是k\*m矩阵，l是m\*1的全1列向量，其中m是样本数，k是样本属性维度。中心化即是每个样本减去中心$\overline{x}$，即$\overline{X}=X-\overline{x}\times l^T=X-(\frac{1}{m}X\times l)\times l^T=X(l-\frac{l}{m}ll^T)=XH$。因此，中心化后的协方差矩阵就是$XHH^TX^T$。

 

## 10.5、降维中涉及的投影矩阵通常要求是正交的，试述正交、非正交投影矩阵用于降维的优缺点。

答:当投影矩阵正交时，任意两种属性都是相互独立的，其中一个的取值不会影响另一个。

但是属性并非全部不相关，比如书上说的，西瓜的体积和重量显然是正相关的，这时两个属性的投影矩阵不正交会有更好的效果。

 