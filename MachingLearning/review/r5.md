# 第05章 神经网络
## 5.2、试述使用图5.2(b)激活函数的神经元与对率回归的联系。（20分）
答：图5.2(b) 函数作为激活函数，相当于将每个神经元视为对率回归分类器，其系数为输入连接的权值w和阈值θ。（10分）

两者的不同之处在于神经元激活函数不一定要使用Sigmoid函数，只要是非线性的可导函数都可以使用。（10分）

# 5.3、对于图5.7中vih，试推导出BP算法中的更新公式(5.13)。（20分）
答：根据梯度下降法，（5分），其中，
，		（5分）
，					（5分）
，			（5分）
因此，得到。

# 5.4、试述式(5.6)中学习率的取值对神经网络训练的影响。（20分）
    答：学习率η控制着梯度下降法的搜索步长。
    如果学习率η过小，每次下降的很慢，使得迭代次数非常多（10分）；
    如果学习率η过大，在后面迭代时会出现震荡，在最小值附近来回波动（10分）。

常把学习率η设置为随迭代次数变化的量，使其随着训练的要求变化而变化（一般是减小）。如刚开始η较大以快速到达到目标值附近，后期η较小以保证收敛稳定。 

# 5.7、根据式(5.18)和(5.19)，试构造一个能解决异或问题的单层RBF神经网络。（40分）
答：RBF是一种单隐层的前馈网络，它使用径向基函数作为隐层神经元激活函数，输出则是对隐层的一个线性组合（5分）。

RBF隐层相当于把输入映射到了一个更高维的空间，使得原本线性不可分的数据变成线可分，所以隐层神经元一般至少比输入层多一个。（5分）

由于是异或问题，所以构造数据集（5分）

x1	x2	y
0	0	0
0	1	1
1	0	1
1	1	0

由此设计能解决最简单异或问题的RBF网络如下：（5分）

输入层：由于有2个输入，输入层有2个神经元；

隐层：隐层神经元越多拟合的越好，设为可变的q个，但至少要比输入层多1个；

输出层：1个神经元。

该网络的参数有：

x, y：样本参数；（10分）

wi：隐层第i个神经元的权值；

ci：隐层第i个神经元的中心，可以通过对x聚类或者从x中随机采样获得；

βi：样本与第i个神经元的中心的距离的缩放系数。

下面确定参数wi和βi：（10分）

定义误差函数
$$
E_k=\frac{1}{2}(\varphi(x^k)-y^k)^2 \ \ \ \ \ \  k=1,2,3,4;\\
\Delta w_i=-\eta(\varphi (x^k)-y^k)\rho(x^k,c_i),\\
\Delta \beta_i=-\eta\frac{\varphi E_k}{\varphi \beta_i}=\eta\omega
$$

；那么，

