# 第05章 神经网络
## 5.2、试述使用图5.2(b)激活函数的神经元与对率回归的联系。（20分）
答：图5.2(b) 函数作为激活函数，相当于将每个神经元视为对率回归分类器，其系数为输入连接的权值$\omega$和阈值$\theta$。（10分）

两者的不同之处在于神经元激活函数不一定要使用Sigmoid函数，只要是非线性的可导函数都可以使用。（10分）

## 5.3、对于图5.7中$V_{hi}$，试推导出BP算法中的更新公式(5.13)。（20分）

答：根据梯度下降法，$\Delta v_{ih}=-\eta\frac{\delta E_k}{\delta v_{ih}}$（5分），其中
$$
\begin{aligned}
\frac{\delta E_k}{v_{ih}}&=\frac{\delta E_k}{\delta b_h}\frac{\delta b_h}{\delta a_h}\frac{\delta a_h}{\delta v_{ih}},\ \ \ \ (5分)\newline
\frac{\delta a_h}{\delta v_{ih}}&=x_i,\ \ \ (5分)\newline
e_h&=-\frac{\delta E_k}{\delta b_h}\frac{\delta b_h}{\delta a_h}\ \ \ \ (5分)
\end{aligned}
$$
因此，得到。

$\Delta v_{ih}=\eta e_h x_i$

## 5.4、试述式(5.6)中学习率的取值对神经网络训练的影响。（20分）

答：学习率η控制着梯度下降法的搜索步长
如果学习率η过小，每次下降的很慢，使得迭代次数非常多（10分）；
如果学习率η过大，在后面迭代时会出现震荡，在最小值附近来回波动（10分）。

常把学习率η设置为随迭代次数变化的量，使其随着训练的要求变化而变化（一般是减小）。如刚开始η较大以快速到达到目标值附近，后期η较小以保证收敛稳定。 

## 5.7、根据式(5.18)和(5.19)，试构造一个能解决异或问题的单层RBF神经网络。（40分）

答：RBF是一种单隐层的前馈网络，它使用径向基函数作为隐层神经元激活函数，输出则是对隐层的一个线性组合（5分）。

RBF隐层相当于把输入映射到了一个更高维的空间，使得原本线性不可分的数据变成线可分，所以隐层神经元一般至少比输入层多一个。（5分）

由于是异或问题，所以构造数据集（5分）

x1	x2	y
0	0	0
0	1	1
1	0	1
1	1	0

由此设计能解决最简单异或问题的RBF网络如下：（5分）

输入层：由于有2个输入，输入层有2个神经元；

隐层：隐层神经元越多拟合的越好，设为可变的q个，但至少要比输入层多1个；

输出层：1个神经元。

该网络的参数有：

x, y：样本参数；（10分）

$w_i$：隐层第i个神经元的权值；

$c_i$：隐层第i个神经元的中心，可以通过对x聚类或者从x中随机采样获得；

$β_i$：样本与第i个神经元的中心的距离的缩放系数。

下面确定参数$w_i$和$β_i$：（10分）

定义误差函数
$$
E_k=\frac{1}{2}(\varphi(x^k)-y^k)^2 \ \ \ \ \ \  k=1,2,3,4;\\
\Delta w_i=-\eta(\varphi (x^k)-y^k)\rho(x^k,c_i),\\
\Delta \beta_i=-\eta\frac{\varphi E_k}{\varphi \beta_i}=\eta\omega
$$

那么

$\Delta \omega_i=-\eta\frac{\delta E_k}{\delta \omega_i}=-\eta(\varphi(x^k)-y^k)\rho(x^k,c_i)$

$\Delta \beta_i=-\eta\frac{\delta E_k}{\delta \beta_i}=-\eta \omega_i(\varphi(x^k-y^k))\rho(x^k,c_i)||x^k-c_i||$

